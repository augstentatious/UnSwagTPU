# UnSwag

```
    _    _       _______
   | |  | |     / ______|
   | |  | |_ __| (___ __      ____ _  __ _
   | |  | | '_ \\___ \\ \ /\ / / _` |/ _` |
   | |__| | | | |___) |\ V  V / (_| | (_| |
    \____/|_| |_|____/  \_/\_/ \__,_|\__, |
                                      __/ |
    .---------------------------.    |___/
    |  [|||||||||] [|||||||||]  |
    |  """"""""""" """""""""""  |__
    `---------------------------'  |
       `---------------------------'

   [!] STATUS: EXPERIMENTAL // ALPHA v0.1.0
   [!] ARCH: DUAL-STACK / (JAX/TPU & TORCH/GPU) 
   [!] TARGET: COMMODITY SILICON
```
"The Memory Wall is a choice." â€” *Sophia Labs*

UnSwag is a memory-efficient training primitive forged in The Clean Room. It maps ReLU activations to 1-bit structural isomorphisms, effectively removing the memory bottleneck for large-context training.

By compressing forward-pass activations into 1-bit packets, we achieve 32x memory reduction with mathematically identical convergence.

## ðŸ¦ The Protocol
**1-Bit Isomorphism**: Compresses activation memory by 32x (vs FP32) with mathematical equivalence.

**Dual-Stack Architecture**:

**TPU (JAX)**: Built for massive context windows (128k+) on Google TPUs.

**GPU (Triton)**: Custom OpenAI Triton kernels for NVIDIA T4/A100/H100 hardware.

**Hardware Agnostic**: Automatically detects silicon and loads the correct firmware.

---

## ðŸ“Š Verified Benchmarks (Gemma-2-9B Scale)

*Tested on TPU v3-8 (16GB VRAM per core)*

| Metric | Standard ReLU | UnSwag (1-Bit) |

| --- | --- | --- |

| **Activation Memory (128k context)** | ~7.30 GB / layer | **~229.00 MB / layer** |

| **Max Stable Context** | ~12k tokens | **131,072 tokens** |

| **Gradient Parity Error** | 0.000000 | **0.000000** |

| **Compression Ratio** | 1x | **32x** |

## ðŸ“¦ Installation
pip install unswag
---

## ðŸš€ Usage
UnSwag automatically detects your accelerator (TPU or GPU) and creates the isomorphism.

**Option A: PyTorch (NVIDIA GPU)**
*Powered by custom Triton Kernels.*
```
from transformers import AutoModelForCausalLM
from unswag import unswag_model

# 1. Load Standard Model
model = AutoModelForCausalLM.from_pretrained("google/gemma-2-9b")

# 2. Inject the Protocol
 Surgically replaces linear layers with 1-bit isomorphic layers
model = unswag_model(model)

# 3. Train
# Activation memory is now 96.8% empty.
```

**Option B: JAX / Flax (Google TPU)**
*Powered by Pallas/XLA.*
```
import jax
from unswag import unswag_relu

@jax.jit
def train_step(w, x):
    # Standard linear pass
    gate = jax.numpy.dot(x, w)
    
    # 1-Bit Activation Caching
    # Automatically packs bits for backward pass
    return unswag_relu(gate)
```
---

## ðŸ§± The 256k Integer Wall
During testing on a TPU v3-8, UnSwag successfully bypassed the memory wall, eventually hitting the **XLA Hardware Addressing Limit**:

* **131,072 Context**: Stable âœ…

* **262,144 Context**: XLA Integer Overflow (3.75B elements) âŒ

ðŸ›¡ï¸ Mathematical Proof: 1-Bit VJP Isomorphism

The "Memory Wall" exists because standard backpropagation requires storing the full activation $h$ of every layer to compute the gradient. For a ReLU layer, the forward pass is:

$$y = \text{ReLU}(W \cdot x + b)$$

The derivative of $\text{ReLU}(z)$ is the Heaviside Step Function $H(z)$:

$$\frac{d}{dz}\text{ReLU}(z) = H(z) = \begin{cases} 1 & z > 0 \\ 0 & z \le 0 \end{cases}$$

Crucially, $H(z)$ is binary. It does not depend on the magnitude of $z$, only its sign.

UnSwag exploits this by storing only the sign bits ($\text{sgn}(z)$) in a bit-packed uint32 array. This reduces the storage for the backward pass from 32 bits per element to **1 bit per element**, a 32x reduction. Because $H(z) \equiv (\text{sgn}(z) > 0)$, the reconstructed gradient is bit-identical to the standard gradient.

**Maintained by Sophia Labs.** *Forged in The Clean Room.*
